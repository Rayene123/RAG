{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ba39e1",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation - Interactive Tutorial\n",
    "\n",
    "This notebook demonstrates how to evaluate your RAG system using RAGAS metrics.\n",
    "\n",
    "## What is RAGAS?\n",
    "\n",
    "RAGAS (RAG Assessment) is a framework to evaluate Retrieval Augmented Generation systems using metrics like:\n",
    "- **Faithfulness**: Answers are grounded in context\n",
    "- **Answer Relevancy**: Answers address the question\n",
    "- **Context Relevancy**: Retrieved contexts are useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e081260",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8806f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath('..')\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from evaluation import RAGASEvaluator, DatasetBuilder, get_ragas_metrics\n",
    "from rag_core.pipeline.query_pipeline import QueryPipeline\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ed2b7",
   "metadata": {},
   "source": [
    "## Step 1: Initialize RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f14be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your RAG pipeline\n",
    "pipeline = QueryPipeline()\n",
    "\n",
    "print(\"‚úÖ RAG Pipeline initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8295896f",
   "metadata": {},
   "source": [
    "## Step 2: Run Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries\n",
    "test_queries = [\n",
    "    \"Find clients with high income and education level\",\n",
    "    \"Show me clients who have defaulted on loans\",\n",
    "    \"Retrieve information about clients with overdue payments\"\n",
    "]\n",
    "\n",
    "# Run queries and collect results\n",
    "query_results = []\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nüîç Query: {query}\")\n",
    "    results = pipeline.execute(query, top_k=3, verbose=False)\n",
    "    query_results.append({\n",
    "        'query': query,\n",
    "        'results': results\n",
    "    })\n",
    "    print(f\"   Retrieved {len(results['results'])} results\")\n",
    "\n",
    "print(\"\\n‚úÖ All queries executed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfce7f6",
   "metadata": {},
   "source": [
    "## Step 3: Build Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset builder\n",
    "builder = DatasetBuilder()\n",
    "\n",
    "# Add each query result to the dataset\n",
    "for item in query_results:\n",
    "    query = item['query']\n",
    "    results = item['results']['results']\n",
    "    \n",
    "    # Extract contexts\n",
    "    contexts = [r['text'] for r in results]\n",
    "    \n",
    "    # Generate a simple answer (in production, use LLM)\n",
    "    answer = f\"Based on the search, I found {len(contexts)} relevant clients. {contexts[0][:200]}...\"\n",
    "    \n",
    "    # Add to dataset\n",
    "    builder.add_sample(\n",
    "        question=query,\n",
    "        answer=answer,\n",
    "        contexts=contexts\n",
    "    )\n",
    "\n",
    "# Build the dataset\n",
    "dataset = builder.build_dataset()\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset built with {len(dataset)} samples\")\n",
    "print(f\"\\nDataset summary: {builder.get_summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af84f1",
   "metadata": {},
   "source": [
    "## Step 4: Run RAGAS Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa65bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAGAS evaluator\n",
    "evaluator = RAGASEvaluator()\n",
    "\n",
    "# Get metrics (without ground truth)\n",
    "metrics = get_ragas_metrics(include_all=False)\n",
    "\n",
    "print(\"üîç Running RAGAS evaluation...\\n\")\n",
    "print(f\"Metrics: {[m.name for m in metrics]}\\n\")\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluator.evaluate_dataset(\n",
    "    dataset=dataset,\n",
    "    metrics=metrics,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f2185",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a2c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame([results])\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìä RAGAS Scores:\")\n",
    "print(\"=\"*50)\n",
    "for metric, score in results.items():\n",
    "    print(f\"{metric:.<30} {score:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results.keys(), results.values(), color='skyblue')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.title('RAGAS Evaluation Results')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.axhline(y=0.7, color='g', linestyle='--', label='Good threshold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf64cd4",
   "metadata": {},
   "source": [
    "## Step 6: Interpret Results\n",
    "\n",
    "### Score Interpretation:\n",
    "- **Faithfulness**: Measures if answers are factual based on context\n",
    "  - >0.8: Excellent - minimal hallucination\n",
    "  - 0.6-0.8: Good\n",
    "  - <0.6: Needs improvement\n",
    "\n",
    "- **Answer Relevancy**: Measures if answers address the question\n",
    "  - >0.7: Excellent\n",
    "  - 0.5-0.7: Good\n",
    "  - <0.5: Needs improvement\n",
    "\n",
    "- **Context Relevancy**: Measures if retrieved contexts are useful\n",
    "  - >0.6: Excellent\n",
    "  - 0.4-0.6: Good\n",
    "  - <0.4: Needs improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c62973d",
   "metadata": {},
   "source": [
    "## Optional: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8921f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "builder.save_to_csv('../evaluation/notebook_test_dataset.csv')\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('../evaluation/notebook_evaluation_results.csv', index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Results saved!\")\n",
    "print(\"   - Dataset: evaluation/notebook_test_dataset.csv\")\n",
    "print(\"   - Results: evaluation/notebook_evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f5512",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Try with more test queries\n",
    "2. Add ground truth answers for full metrics\n",
    "3. Compare different configurations (top_k, filters, etc.)\n",
    "4. Integrate LLM for better answer generation\n",
    "5. Set up automated evaluation pipeline\n",
    "\n",
    "See `RAGAS_INTEGRATION_GUIDE.md` for more details!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
